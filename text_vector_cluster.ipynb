{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.53 0.   0.53 0.   0.66]\n",
      " [0.   0.   0.63 0.   0.78]\n",
      " [0.54 0.   0.   0.84 0.  ]\n",
      " [0.47 0.74 0.47 0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    " \n",
    "docs = np.array([\n",
    "        'バナナ リンゴ みかん',      # 文書１\n",
    "        'バナナ リンゴ',      # 文書２\n",
    "        'モモ みかん',   # 文書３\n",
    "        'みかん バナナ ナシ'            # 文書４\n",
    "        ])\n",
    "\n",
    "# vectorizer = TfidfVectorizer(use_idf=True, token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "vecs = vectorizer.fit_transform(docs)\n",
    "print(vecs.toarray())\n",
    "\n",
    "# 行方向に文書、列方向に単語が並んでいます。\n",
    "# 単語と列の対応は下記で確認できます。\n",
    "# 上記サンプルで、token_pattern=u'(?u)\\\\b\\\\w+\\\\b’ を指定しました。これは、文字列長が 1 の単語を処理対象に含めることを意味します。\n",
    "# この指定をはずすと、長さ一文字の単語がまったくカウントされなくなります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "みかん 0\n",
      "ナシ 1\n",
      "バナナ 2\n",
      "モモ 3\n",
      "リンゴ 4\n"
     ]
    }
   ],
   "source": [
    "for k,v in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1]):\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.32 0.   0.   0.8  0.51 0.  ]\n",
      " [0.   0.   0.41 0.65 0.   0.   0.65 0.  ]\n",
      " [0.   0.   0.41 0.65 0.   0.   0.   0.65]\n",
      " [0.69 0.   0.38 0.   0.   0.   0.   0.61]\n",
      " [0.   0.   0.32 0.   0.8  0.   0.51 0.  ]\n",
      " [0.49 0.57 0.27 0.43 0.   0.   0.   0.43]\n",
      " [0.69 0.   0.38 0.   0.   0.   0.61 0.  ]\n",
      " [0.   0.49 0.24 0.75 0.   0.   0.   0.37]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    " \n",
    "docs = np.array([\n",
    "        '牛乳 を 買う',\n",
    "        'パン を 買う',\n",
    "        'パン を 食べる',\n",
    "        'お菓子 を 食べる',\n",
    "        '本 を 買う',\n",
    "        'パン と お菓子 を 食べる',\n",
    "        'お菓子 を 買う',\n",
    "        'パン と パン を 食べる'\n",
    "        ])\n",
    "#\n",
    "# ベクトル化\n",
    "#\n",
    "vectorizer = TfidfVectorizer(use_idf=True, token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\n",
    "vecs = vectorizer.fit_transform(docs)\n",
    " \n",
    "print(vecs.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 牛乳 を 買う\n",
      "0 パン を 買う\n",
      "1 パン を 食べる\n",
      "1 お菓子 を 食べる\n",
      "0 本 を 買う\n",
      "1 パン と お菓子 を 食べる\n",
      "0 お菓子 を 買う\n",
      "1 パン と パン を 食べる\n"
     ]
    }
   ],
   "source": [
    "clusters = KMeans(n_clusters=2, random_state=0).fit_predict(vecs)\n",
    "for doc, cls in zip(docs, clusters):\n",
    "    print(cls, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
